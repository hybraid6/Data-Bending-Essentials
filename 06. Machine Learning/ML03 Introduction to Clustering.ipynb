{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffea4539",
   "metadata": {},
   "source": [
    "# Introduction to Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f567a20",
   "metadata": {},
   "source": [
    "**Clustering** is an unsupervised machine learning technique where the goal is to divide a dataset into groups, or clusters, such that the data points within each group are more similar to each other than to those in other groups. We'll look at 3 types: **K-means**, **Hierarchical** and **DBSCAN Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3238c274",
   "metadata": {},
   "source": [
    "## 1. K-Means Clustering\n",
    "- **How it works:** \n",
    "Divides data into ùêæ clusters by minimizing the sum of squared distances between data points and their corresponding cluster centroids.\n",
    "- **Strengths:**\n",
    "    - Fast and efficient on large datasets.\n",
    "    - Works well when clusters are spherical and well-separated.\n",
    "- **Weaknesses:**\n",
    "    - Requires specifying ùêæ in advance.\n",
    "    - Sensitive to outliers and initial cluster centroids.\n",
    "    - Performs poorly with non-spherical clusters or varying cluster densities.\n",
    "- **Best for:** Clustering when data is roughly spherical, and the number of clusters is known or can be estimated.\n",
    "\n",
    "**More:** https://www.youtube.com/watch?v=4b5d3muPQmA&t=230s\n",
    "\n",
    "**Key Terms**:\n",
    "- **Centroid**: The center of a cluster, used in algorithms like K-Means.\n",
    "- **Inertia**: A measure of how tightly clustered the points in a cluster are (also known as within-cluster sum of squares).\n",
    "- **Euclidean Distance**: The straight-line distance between two points, often used in K-Means to assign points to the nearest cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad22c8a9",
   "metadata": {},
   "source": [
    "### In Detail:\n",
    "    \n",
    "1.  **The centroid** is calculated as the **mean** of all data points (or vectors) within the cluster for each feature (or dimension). In short, the centroid is the average position of all the points in the cluster.\n",
    "\n",
    "    If you have 3 data points in a cluster with two features (e.g., income and spending score), the centroid for each feature is the average of the feature values of the 3 points.\n",
    "\n",
    "    For example, if the points are:\n",
    "    - Point 1: (50, 60)\n",
    "    - Point 2: (70, 80)\n",
    "    - Point 3: (60, 90)\n",
    "\n",
    "    The centroid would be:\n",
    "    - $\n",
    "    \\text{Centroid for Feature 1 (Income)} = \\frac{50 + 70 + 60}{3} = 60\n",
    "    $\n",
    "    - $\n",
    "    \\text{Centroid for Feature 2 (Spending Score)} = \\frac{60 + 80 + 90}{3} = 76.67\n",
    "    $\n",
    "\n",
    "    So the centroid would be at $ (60, 76.67) $.\n",
    "___    \n",
    "    \n",
    "2. **Inertia** in K-Means clustering is a measure of how well the clustering algorithm has grouped the data.The lower the inertia, the better the clustering (as long as it's not overfitting).\n",
    "    - Inertia decreases as the number of clusters increases, because with more clusters, the data points are closer to their respective centroids. However, if you use too many clusters, inertia may get very low, but the clusters might become too small and lose meaning.\n",
    "    - In practice, we use the *Elbow Method* to find the optimal number of clusters where the decrease in inertia slows down, indicating that adding more clusters doesn‚Äôt significantly improve the clustering.\n",
    "\n",
    "___\n",
    "\n",
    "3. **Euclidean Distance** is the shortest distance between two points in space.\n",
    "\n",
    "   In K-Means clustering, the Euclidean distance is used to:\n",
    "    - Assign data points to clusters: Each data point is assigned to the cluster whose centroid is closest, where closeness is measured by the Euclidean distance.\n",
    "    - Update the centroids: Once the points are assigned, the centroids are updated based on the mean of all points in each cluster, and the process repeats.\n",
    "    \n",
    "  - However, Euclidean distance assumes that all features are on the same scale. If some features have larger values or more variation, they can dominate the distance calculation. That's why it's common to standardize or normalize the data (e.g., using `StandardScaler`) before using Euclidean distance, especially in algorithms like K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2782e0f0",
   "metadata": {},
   "source": [
    "### Mall Customer Segmentation using the K-Means algorithm\n",
    "\n",
    "The goal here is to cluster customers into different groups based on their purchasing behavior, demographics, or preferences, allowing businesses to tailor marketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # Import NumPy for numerical operations\n",
    "import pandas as pd  # Import Pandas for data manipulation\n",
    "import matplotlib.pyplot as plt  # Import Matplotlib for plotting\n",
    "import seaborn as sns  # Import Seaborn for enhanced plotting capabilities\n",
    "from sklearn.cluster import KMeans  # Import KMeans from scikit-learn for clustering\n",
    "from sklearn.preprocessing import StandardScaler  # Import StandardScaler from scikit-learn for data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8bb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (Mall Customer Segmentation Data)\n",
    "\n",
    "data = pd.read_csv(\"Mall_Customers.csv\")\n",
    "\n",
    "# Inspect the first few rows of the dataset to understand its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b5a90",
   "metadata": {},
   "source": [
    " **`Annual Income (k$)` and `Spending Score (1-100)` are the features we'll use to cluster customers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f639f7a6",
   "metadata": {},
   "source": [
    "### Check for missing values\n",
    "This is to ensure data quality. Missing values can affect clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ddb1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98640175",
   "metadata": {},
   "source": [
    "No missing value, cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a8456f",
   "metadata": {},
   "source": [
    "### Extract relevant features for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b79e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['Annual Income (k$)', 'Spending Score (1-100)']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee59328",
   "metadata": {},
   "source": [
    "### Standardizing the data\n",
    "Standardization is important because clustering algorithms like K-Means are sensitive to feature scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fddd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ed940",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  # Initialize the StandardScaler to standardize the features by removing the mean and scaling to unit variance\n",
    "X_scaled = scaler.fit_transform(X)  # Fit the scaler to the data and then transform the features, returning the standardized version of the dataset 'X'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01e1a72",
   "metadata": {},
   "source": [
    "- `StandardScaler()` adjusts the data so that each feature has `mean=0` and `standard deviation=1`.\n",
    "- This step ensures that one feature (like income) doesn't dominate others due to its larger scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d962e3",
   "metadata": {},
   "source": [
    "### Using the Elbow method to find the optimal number of clusters\n",
    "The **Elbow Method** helps to identify the best number of clusters for K-Means by plotting inertia vs. the number of clusters.\n",
    "- Similar to hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4bbccb",
   "metadata": {},
   "source": [
    "The **Elbow Method** is a simple technique used to determine the optimal number of clusters in K-Means clustering.\n",
    "\n",
    "- **How It Works:**\n",
    "    1. **Run K-Means**: You run K-Means clustering on your data with different values for the number of clusters (k), typically starting from 1 and going up to 10 or more. \n",
    "    2. **Calculate the Inertia or Within-Cluster Sum of Squares (WCSS)**: For each value of k, calculate the sum of the squared distances between data points and the centroid of their assigned cluster. This is called the **WCSS** or **inertia**. It measures how tightly data points are grouped within each cluster‚Äîthe lower the value, the better the clustering.\n",
    "    3. **Plot the inertia**: Plot the number of clusters (k) on the x-axis and the inertia on the y-axis.\n",
    "    4. **Look for the Elbow**: As k increases, the inertia decreases. Initially, the inertia drops sharply as you add more clusters, but after a certain point, the decrease becomes gradual. The \"elbow\" in the graph is where the rate of decrease slows down, indicating that adding more clusters doesn't significantly improve the clustering.\n",
    "\n",
    "- **Why It's Called the Elbow Method:**\n",
    "    - The \"elbow\" is the point in the graph where the inertia curve bends, resembling the shape of an elbow.\n",
    "    - The elbow represents a good trade-off: It suggests the ideal number of clusters before adding more clusters stops being useful.\n",
    "\n",
    "- **Simple Example:**\n",
    "Imagine you want to group people in a room into clusters. Initially, having just one cluster groups everyone together, and the distance between people and their centroid is large. As you increase the number of clusters, people are grouped more closely together, and the distance decreases. But after a certain point, adding more groups doesn't make a big difference‚Äîthe elbow shows the sweet spot for how many groups you should use.\n",
    "\n",
    "- In short, the Elbow Method helps you find the right balance for choosing the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f89073",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia_list = []  # List to store the inertia values for each number of clusters\n",
    "\n",
    "for i in range(1, 11):  # Loop over cluster numbers from 1 to 10\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)  # Initialize KMeans with the current number of clusters (i)\n",
    "    kmeans.fit(X_scaled)  # Fit the KMeans model on the standardized data\n",
    "    inertia_list.append(kmeans.inertia_)  # Append the inertia (sum of squared distances) to the list\n",
    "\n",
    "# Plot the Elbow curve\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size for better visibility\n",
    "plt.plot(range(1, 11), inertia_list, 'ro-')  # Plot the number of clusters against inertia with red markers and lines\n",
    "# 'r': Specifies the color of the plot, in this case, red (r stands for red).\n",
    "# 'o': Specifies the marker style, which in this case is a circle (o).\n",
    "# '-': Specifies the line style, which in this case is a solid line (-).\n",
    "\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')  # Add a title to the plot\n",
    "plt.xlabel('Number of Clusters')  # Label the x-axis as 'Number of Clusters'\n",
    "plt.ylabel('Inertia')  # Label the y-axis as 'Inertia' (within-cluster sum of squares)\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e17af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c902a2e",
   "metadata": {},
   "source": [
    "#### In this case, we can choose 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18242c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit K-Means with 5 clusters\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)  # Initialize the KMeans algorithm with 5 clusters and a fixed random state for reproducibility\n",
    "kmeans.fit(X_scaled)  # Fit the KMeans model to the standardized data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e5170",
   "metadata": {},
   "source": [
    "### Add the cluster labels to the original dataset for interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48bfc4-1ee7-436e-9547-c02fece7e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60462aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60386b1-5f07-4f1d-9272-2b84a7a0e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e309db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cluster'] = kmeans.labels_  # Assign the cluster labels from the KMeans model to a new column 'Cluster' in the dataset\n",
    "\n",
    "# Visualize the clusters using a scatter plot\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size for better visibility\n",
    "sns.scatterplot(x=data['Annual Income (k$)'], y=data['Spending Score (1-100)'], hue=data['Cluster'], palette='Set1')  \n",
    "# Create a scatter plot with 'Annual Income' on the x-axis, 'Spending Score' on the y-axis, and color the points by their cluster labels\n",
    "plt.title('Customer Segments based on Annual Income and Spending Score')  # Add a title to the plot\n",
    "plt.xlabel('Annual Income (k$)')  # Label the x-axis\n",
    "plt.ylabel('Spending Score (1-100)')  # Label the y-axis\n",
    "plt.legend()  # Display the legend showing the clusters\n",
    "plt.show()  # Show the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431d3e44-42fb-4437-974b-30716bdca035",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d95b2",
   "metadata": {},
   "source": [
    "- Each point in the scatter plot represents a customer, with color indicating which cluster they belong to.\n",
    "- You can now interpret the clusters to understand different customer segments (e.g., high-income and low-spending vs. low-income and high-spending customers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cedd5bc",
   "metadata": {},
   "source": [
    "### Practice\n",
    "1. Modify the code to include 'Age' as a feature in addition to 'Annual Income' and 'Spending Score.' How do the clusters change?\n",
    "2. Try tuning the K-Means algorithm with a different number of clusters and compare your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea834f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765f2401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00211c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "data['Cluster'] = kmeans.labels_\n",
    "# Create 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    data,\n",
    "    x='Annual Income (k$)',\n",
    "    y='Spending Score (1-100)',\n",
    "    z='Age',\n",
    "    color = 'Cluster',\n",
    "    labels={\"Annual Income (k$)\": \"Annual Income (k$)\", \"Spending Score (1-100)\": \"Spending Score (1-100)\", \"Age\": \"Age\"},\n",
    "    width=600,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "# Refine formatting\n",
    "fig.update_traces(\n",
    "    marker={\"size\": 4, \"line\": {\"width\": 2, \"color\": \"DarkSlateGrey\"}},\n",
    "    selector={\"mode\": \"markers\"},\n",
    ")\n",
    "\n",
    "# Display figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73870f5",
   "metadata": {},
   "source": [
    "## 2. Hierarchical Clustering\n",
    "- **How it works:** Builds a hierarchy of clusters by either:\n",
    "    - **Agglomerative (Bottom-up approach):** Starting with individual points and merging them into larger clusters.\n",
    "    - **Divisive (Top-down approach):** Starting with the entire dataset and splitting it into smaller clusters.\n",
    "- **Strengths:**\n",
    "    - No need to predefine ùêæ.\n",
    "    - More flexible with the number of clusters and provides a clear visualization of how clusters are formed (via the dendrogram).\n",
    "- **Weaknesses:**\n",
    "    - Computationally expensive for large datasets.\n",
    "    - Can be sensitive to noise.\n",
    "- **Best for:** \n",
    "Datasets where you want a hierarchical structure of clusters or you don‚Äôt know the number of clusters in advance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4db36a",
   "metadata": {},
   "source": [
    "###  Mall Customer Segmentation using Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce5e325",
   "metadata": {},
   "source": [
    "**Agglomerative Clustering** is a bottom-up approach to hierarchical clustering where each data point starts in its own cluster, and clusters are merged iteratively based on a linkage criterion until a single cluster or a specified number of clusters is achieved.\n",
    "\n",
    "#### **Key Terms**:\n",
    "- **Bottom-Up Approach**: Each data point starts as an individual cluster and merges with the closest clusters.\n",
    "- **Linkage Criteria**: Determines how the distance between clusters is calculated. Common types are:\n",
    "  - **Single Linkage**: Minimum distance between points in two clusters.\n",
    "  - **Complete Linkage**: Maximum distance between points in two clusters.\n",
    "  - **Average Linkage**: Average distance between points in two clusters.\n",
    "  - **Ward's Method**: Minimizes the variance within clusters during merging.\n",
    "- **Dendrogram**: A tree-like diagram that shows the hierarchy of cluster merges.\n",
    "\n",
    "- **More :** https://www.youtube.com/watch?v=ijUMKMC4f9I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bcd996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # Import NumPy for numerical operations\n",
    "import pandas as pd  # Import Pandas for data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # Import Matplotlib for creating plots and visualizations\n",
    "import seaborn as sns  # Import Seaborn for enhanced data visualization based on Matplotlib\n",
    "from sklearn.preprocessing import StandardScaler  # Import StandardScaler for normalizing or scaling features before clustering\n",
    "from sklearn.cluster import AgglomerativeClustering  # Import AgglomerativeClustering for hierarchical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage  # Import dendrogram and linkage for visualizing hierarchical clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (Mall Customer Segmentation Data)\n",
    "data = pd.read_csv(\"Mall_Customers.csv\")\n",
    "\n",
    "# Inspect the first few rows of the dataset to understand its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab3363b",
   "metadata": {},
   "source": [
    "#### We will also use 'Annual Income (k$)' and 'Spending Score (1-100)' as the features for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0524961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant features\n",
    "X = data[['Annual Income (k$)', 'Spending Score (1-100)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48557417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Standardizing ensures that all features are on the same scale, which is important for distance-based clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c473b626",
   "metadata": {},
   "source": [
    "### Step 1: Visualize the Hierarchy using a Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d27d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scipy's linkage method to perform hierarchical/agglomerative clustering\n",
    "# 'ward' is the linkage method that minimizes variance within clusters\n",
    "linked = linkage(X_scaled, method='ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948a641",
   "metadata": {},
   "source": [
    "**Ward's method** is a popular choice in hierarchical (agglomerative) clustering due to its ability to create compact, evenly-sized clusters by minimizing the within-cluster variance at each merging step. Its focus on variance makes it well-suited for applications where you expect clusters of roughly equal size and spherical shapes.\n",
    "\n",
    "- **Ward's Method vs. Other Linkage Criteria**\n",
    "    - **Single Linkage:** Merges clusters based on the shortest distance between any two points in different clusters (can create elongated clusters).\n",
    "    - **Complete Linkage:** Merges clusters based on the maximum distance between any two points in different clusters (produces more compact clusters than single linkage but is more sensitive to outliers).\n",
    "    - **Average Linkage:** Merges clusters based on the average distance between all pairs of points in different clusters.\n",
    "    - **Ward's Linkage:** Merges clusters based on minimizing the variance (sum of squared deviations), producing compact and equally sized clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcccef7c",
   "metadata": {},
   "source": [
    "#### Create a dendrogram to visualize the cluster hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47494d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure with a specific size for the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot the dendrogram using the linkage matrix (linked), where:\n",
    "# - orientation='top' means the root of the dendrogram will be at the top.\n",
    "# - distance_sort='descending' sorts clusters by descending distances between them.\n",
    "# - show_leaf_counts=True will display the number of points in each cluster.\n",
    "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "\n",
    "# Add a title to the plot to describe it as a dendrogram for agglomerative clustering\n",
    "plt.title('Dendrogram for Agglomerative Clustering')\n",
    "\n",
    "# Label the x-axis as 'Data Points' since the horizontal axis represents the individual data points or clusters\n",
    "plt.xlabel('Data Points')\n",
    "\n",
    "# Label the y-axis as 'Euclidean Distance' because the vertical axis represents the distance between merged clusters\n",
    "plt.ylabel('Euclidean Distance')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516c874",
   "metadata": {},
   "source": [
    "The dendrogram provides a visual representation of how clusters are formed and merged. The height of the lines indicates the distance (dissimilarity) between clusters when they are merged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6c46b",
   "metadata": {},
   "source": [
    "### Step 2: Apply Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016df7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform agglomerative clustering with the following parameters:\n",
    "# - n_clusters=5: Specifies that we want to group the data into 5 clusters.\n",
    "# - metric='euclidean': Specifies that the Euclidean distance will be used to measure the similarity between data points.\n",
    "# - linkage='ward': Specifies that Ward‚Äôs method will be used to minimize the variance within clusters when merging them.\n",
    "agglom = AgglomerativeClustering(n_clusters=5, metric='euclidean', linkage='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict the clusters\n",
    "clusters = agglom.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to the original dataset\n",
    "data['Cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb20e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters using a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=data['Annual Income (k$)'], y=data['Spending Score (1-100)'], hue=data['Cluster'], palette='Set1')\n",
    "plt.title('Customer Segments based on Annual Income and Spending Score (Agglomerative Clustering)')\n",
    "plt.xlabel('Annual Income (k$)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962dc0e",
   "metadata": {},
   "source": [
    "The scatter plot shows the clusters identified by the Agglomerative Clustering algorithm based on annual income and spending score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Practice Questions\n",
    "\n",
    "#  Modify the code to use a different linkage method, such as single linkage or complete linkage. How does the clustering result change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcfcfa1",
   "metadata": {},
   "source": [
    "## 3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e678b5",
   "metadata": {},
   "source": [
    "**DBSCAN** forms clusters based on how dense the data points are in space. It's great for detecting arbitrary-shaped clusters and handling noise, making it useful for real-world data with irregular cluster shapes or outliers.\n",
    "\n",
    "**How DBSCAN Works:**\n",
    "- **Core Points:** The algorithm looks for points that have many neighboring points within a certain distance. These are called core points because they are in dense regions.\n",
    "\n",
    "- **Neighborhood (Œµ or epsilon):** This is a defined radius around each point. If a point has enough neighbors (determined by a parameter called min_samples) within this radius, it becomes a core point.\n",
    "\n",
    "- **Clusters:** Core points that are close to each other are grouped together to form clusters.\n",
    "\n",
    "- **Border Points:** These are points that are not core points but are within the neighborhood of a core point. They belong to the same cluster as the core point.\n",
    "\n",
    "- **Noise Points:** If a point is neither a core point nor a border point, it‚Äôs considered noise or an outlier and doesn‚Äôt belong to any cluster.\n",
    "\n",
    "#### **Key Terms**:\n",
    "- **Epsilon (Œµ)**: The maximum distance between two points to be considered as neighbors.\n",
    "- **MinPts**: The minimum number of points required to form a dense region (a cluster).\n",
    "- **Core Point**: A point with at least `MinPts` within its `Œµ`-neighborhood.\n",
    "- **Border Point**: A point that is within `Œµ` distance of a core point but does not have enough points to be a core point itself.\n",
    "- **Noise Point**: A point that is neither a core point nor a border point.\n",
    "___\n",
    "\n",
    "- **More:** https://www.youtube.com/watch?v=RDZUdRSDOok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a824ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (Mall Customer Segmentation Data)\n",
    "\n",
    "data = pd.read_csv(\"Mall_Customers.csv\")\n",
    "\n",
    "# Inspect the first few rows of the dataset to understand its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef11ced",
   "metadata": {},
   "source": [
    "Again, We will use 'Annual Income (k$)' and 'Spending Score (1-100)' as the features for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant features\n",
    "X = data[['Annual Income (k$)', 'Spending Score (1-100)']]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Standardizing ensures all features are on the same scale, which is crucial for DBSCAN as it relies on distance measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc15e6",
   "metadata": {},
   "source": [
    "#### Perform DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5184672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN parameters: epsilon (Œµ) and min_samples (MinPts)\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "clusters = dbscan.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to the original dataset\n",
    "data['Cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters using a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=data['Annual Income (k$)'], y=data['Spending Score (1-100)'], hue=data['Cluster'], palette='Set1')\n",
    "plt.title('Customer Segments based on Annual Income and Spending Score (DBSCAN)')\n",
    "plt.xlabel('Annual Income (k$)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Explanation:\n",
    "# In the scatter plot, different colors represent different clusters identified by DBSCAN. Noise points are usually labeled as -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ccb489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of clustering results\n",
    "print(\"Number of clusters found:\", len(set(clusters)) - (1 if -1 in clusters else 0)) # Excluding noise (-1)\n",
    "print(\"Number of noise points:\", list(clusters).count(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b61f8",
   "metadata": {},
   "source": [
    "- **DBSCAN** identifies clusters based on the density of points. It defines clusters as areas of high density separated by areas of low density. Points are classified as core, border, or noise based on their density and proximity.\n",
    "- **Epsilon (Œµ)**: Determines the radius of the neighborhood around each point. A smaller Œµ can lead to many noise points, while a larger Œµ might merge distinct clusters.\n",
    "- **MinPts**: Defines the minimum number of points required to form a dense region. A higher MinPts value might lead to fewer clusters and more noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a795a2f9",
   "metadata": {},
   "source": [
    "### Comparison with K-Means and Hierarchical Clustering:\n",
    "\n",
    "- **DBSCAN**:\n",
    "  - **Advantages**: No need to specify the number of clusters, identifies clusters of arbitrary shape, handles noise well.\n",
    "  - **Disadvantages**: Requires careful parameter tuning (`eps` and `min_samples`), performance can degrade with very large datasets.\n",
    "\n",
    "- **K-Means**:\n",
    "  - **Advantages**: Simple and efficient, works well with spherical clusters.\n",
    "  - **Disadvantages**: Requires specifying the number of clusters, sensitive to the initial placement of centroids.\n",
    "\n",
    "- **Hierarchical Clustering**:\n",
    "  - **Advantages**: No need to specify the number of clusters, provides a dendrogram for visualizing the clustering process.\n",
    "  - **Disadvantages**: Computationally expensive for large datasets, less effective for very large or high-dimensional data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60145e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99b99cb3-e9d0-4174-85f6-4014c25b4451",
   "metadata": {},
   "source": [
    "---\n",
    "_**Your Dataness**_,  \n",
    "**`Obinna Oliseneku`** (_**Hybraid**_)  \n",
    "**[LinkedIn](https://www.linkedin.com/in/obinnao/)** | **[GitHub](https://github.com/hybraid6)**  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
