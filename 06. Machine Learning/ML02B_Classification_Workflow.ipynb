{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096b5d15",
   "metadata": {},
   "source": [
    "# Classification Workflow: Loan Creditworthiness Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f9a5d",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook demonstrates a comprehensive classification workflow for predicting loan creditworthiness based on various features such as income, credit history, and property area. \n",
    "The dataset contains loan application data with 13 features, including categorical (e.g., Gender, Education) and numerical (e.g., ApplicantIncome, LoanAmount) variables.\n",
    "\n",
    "- **Goal:** Build a classification model to predict whether a loan application will be approved (Loan_Status) using the given features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c02f3e1-bdee-4898-a4dd-f73c9426e407",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "This section includes visualizations and insights to **understand the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a935e",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ffccc-830d-4905-981b-940cfee8ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b582b-8c6d-4d5d-8480-f4160fc0691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287a72c-4805-488b-a1b1-9d0966460e9c",
   "metadata": {},
   "source": [
    "Basically, there's a mismatch between the Python environment where `category_encoders` was installed and this Jupyter kernel, so I had to install `category_encoders` in the correct Python environment being used by Jupyter. I did this using `\"correct_filepath\" -m pip install category_encoders` in my command prompt. (Where correct_filepath = the output of `sys.executable`).\n",
    "\n",
    "- **In essence you can ignore the previous 2 cells, you hopefully won't need it** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc576b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncooment the following to install (First time only)\n",
    "# !pip install category_encoders \n",
    "# for target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  #For data manipulation analysis\n",
    "import numpy as np  #For numerical operations\n",
    "import matplotlib.pyplot as plt #For visualization\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import category_encoders as ce                         #For target encoding\n",
    "from sklearn.model_selection import train_test_split   #For splitting the data into train and test sets \n",
    "from sklearn.preprocessing import StandardScaler     #For scaling numerical features\n",
    "\n",
    "# For Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4054a15",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b32e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Train.csv')\n",
    "test_data = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a51714",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891950eb",
   "metadata": {},
   "source": [
    "### Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a brief summary of the data\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10cd0ff",
   "metadata": {},
   "source": [
    "We can see from the non-Null Count that there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e81cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data['Loan_Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad24c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numeric columns\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931fa411",
   "metadata": {},
   "source": [
    "- Several columns like `Gender` and `Married` are in the wrong datatype\n",
    "- From the mean and IQR distribution we see that they are also largely imbalanced. Either mostly 1 or mostly 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed71f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Statistical summary of objects\n",
    "train_data.describe(include = 'object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec2edc",
   "metadata": {},
   "source": [
    "- We'll have to use target encoding for `Loan_ID` when encoding since there're too many unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4498d44",
   "metadata": {},
   "source": [
    "#### Datatype adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e526c806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert relevant columns to categorical data types so we can do relevant EDA on them\n",
    "train_data['Gender'] = train_data['Gender'].astype('object')\n",
    "train_data['Married'] = train_data['Married'].astype('object')\n",
    "train_data['Education'] = train_data['Education'].astype('object')\n",
    "train_data['Self_Employed'] = train_data['Self_Employed'].astype('object')\n",
    "train_data['Credit_History'] = train_data['Credit_History'].astype('object')\n",
    "train_data['Property_Area'] = train_data['Property_Area'].astype('object')\n",
    "\n",
    "# Confirm the data types\n",
    "print(train_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c928c2",
   "metadata": {},
   "source": [
    "#### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3747f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Categorical Variables Univariate Analysis\n",
    "categorical_columns = train_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Loop through categorical columns for count plots\n",
    "for col in categorical_columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(data=train_data, x=col)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb88b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numerical Variables Univariate Analysis\n",
    "numerical_columns = train_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Loop through numerical columns for histograms\n",
    "for col in numerical_columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(train_data[col], bins=30, kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527911f",
   "metadata": {},
   "source": [
    "#### Bivariate Analysis\n",
    "Here, we’ll examine relationships between `Loan_Status` and each of the other features. This will give us insights into how each feature might influence the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599cfda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bivariate Analysis with Categorical Variables vs Target Variable\n",
    "for col in categorical_columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(data=train_data, x=col, hue='Loan_Status')\n",
    "    plt.title(f'{col} vs Loan_Status')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb27f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis with Numerical Variables vs Target Variable\n",
    "for col in numerical_columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(data=train_data, x='Loan_Status', y=col)\n",
    "    plt.title(f'{col} vs Loan_Status')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f222e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numerical features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(train_data[numerical_columns].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994b4bf",
   "metadata": {},
   "source": [
    "## 3. Preprocessing and Feature Engineering\n",
    "This section covers data cleaning, handling missing values, and preparing the dataset for analysis as well as creating new features and selecting the most relevant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b35f94",
   "metadata": {},
   "source": [
    "#### Deal with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd7114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of columns to check for outliers\n",
    "columns_to_check = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Total_Income']\n",
    "\n",
    "# Plot boxplots for each column\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(columns_to_check, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.boxplot(data=train_data, y=col)\n",
    "    plt.title(f\"Boxplot of {col}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a9998d-c594-4e0a-a686-8fd77dc456bf",
   "metadata": {},
   "source": [
    "#### Doesn't help, so we ignore"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31653fb3",
   "metadata": {},
   "source": [
    "# Define thresholds for outliers\n",
    "thresholds = {\n",
    "    'ApplicantIncome': 40000,\n",
    "    'CoapplicantIncome': 10000,  \n",
    "    'LoanAmount': 500,  \n",
    "    'Total_Income': 15000  \n",
    "}\n",
    "\n",
    "# Drop rows with outliers\n",
    "for col, thresh in thresholds.items():\n",
    "    train_data = train_data.drop(train_data[train_data[col] > thresh].index)\n",
    "\n",
    "# Display the updated dataset shape\n",
    "print(\"Dataset shape after removing outliers:\", train_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c581e",
   "metadata": {},
   "source": [
    "### Target Encoding on Loan_ID\n",
    "- Target encoding can be helpful for categorical variables with a high cardinality, like Loan_ID in this case (since Loan_ID has too many unique values for one hot encoding).\n",
    "- In target encoding, each category in Loan_ID will be replaced with the mean target value (Loan_Status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9622ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Target Encoder for Loan_ID\n",
    "target_encoder = ce.TargetEncoder(cols=['Loan_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the encoder on the training set and transform both train and test sets\n",
    "train_data['Loan_ID_encoded'] = target_encoder.fit_transform(train_data['Loan_ID'], train_data['Loan_Status'])\n",
    "test_data['Loan_ID_encoded'] = target_encoder.transform(test_data['Loan_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdfb2e4",
   "metadata": {},
   "source": [
    "### Combine Train and Test Data for Preprocessing\n",
    "To ensure consistency in preprocessing, we'll combine the train and test sets, apply preprocessing, and split them back afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a70519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a source column so that we can easily split back later\n",
    "train_data['source'] = 'train'\n",
    "test_data['source'] = 'test'\n",
    "\n",
    "# Combine train and test sets\n",
    "X_combined = pd.concat([train_data.drop(columns=['Loan_Status']), test_data])\n",
    "\n",
    "# Separate the target variable from features\n",
    "y = train_data['Loan_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c4fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4908a23",
   "metadata": {},
   "source": [
    "### Create `TotalIncome` and `IncomeRatio` Columns\n",
    "- Summing `ApplicantIncome` and `CoapplicantIncome` gives a combined financial profile of the borrower, which can be a useful indicator of total earning potential.\n",
    "- The ratio provides insight into the relative contributions of the primary applicant versus the co-applicant. For instance, a very high ratio (where `ApplicantIncome` dominates) could signal a single-income household, while a balanced ratio might indicate both parties contribute meaningfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bdd2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TotalIncome as the sum of ApplicantIncome and CoapplicantIncome\n",
    "X_combined['TotalIncome'] = X_combined['ApplicantIncome'] + X_combined['CoapplicantIncome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7834dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IncomeRatio, handling cases where CoapplicantIncome might be zero\n",
    "X_combined['IncomeRatio'] = X_combined['ApplicantIncome'] / (X_combined['CoapplicantIncome'] + 1e-5)\n",
    "# The 1e-5 is added to avoid division by zero in some entries in `CoapplicantIncome`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a6de5",
   "metadata": {},
   "source": [
    "### Create a `FamilySize` feature by combining `Dependents` and `Married`\n",
    "The idea is that if `Married` is 1, it implies there is a spouse, so the `FamilySize` is `Dependents` + 2. If `Married` is 0, there’s no spouse, so `FamilySize` would be `Dependents` + 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968353a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_combined['Dependents'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da934d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '3+' with 3 and convert Dependents to integer\n",
    "X_combined['Dependents'] = X_combined['Dependents'].replace('3+', 3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5410a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FamilySize feature based on Dependents and Married columns\n",
    "X_combined['FamilySize'] = X_combined['Dependents'] + 1  # Adding 1 for the applicant\n",
    "X_combined['FamilySize'] += X_combined['Married']  # Add 1 if married (indicating a spouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d738bb4",
   "metadata": {},
   "source": [
    "### Convert `Loan_Amount_Term` from months to Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc119c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined['Loan_Amount_Term_Years'] = X_combined['Loan_Amount_Term'] / 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f1f6e3",
   "metadata": {},
   "source": [
    "### Create `Debt-to-Income` Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9aece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined['Debt_to_Income_Ratio'] = (X_combined['LoanAmount'] / X_combined['TotalIncome']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a425d",
   "metadata": {},
   "source": [
    "### Create `Income_to_Loan` Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4dd7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction: Applicant Income divided by Loan Amount\n",
    "X_combined['Income_to_Loan_Ratio'] = (X_combined['ApplicantIncome'] / X_combined['LoanAmount']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcfa0f",
   "metadata": {},
   "source": [
    "### Create `Income_Per_Person` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ecb789",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined['FamilySize'] = X_combined['FamilySize'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5cd77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction: Total Income multiplied by Household Size\n",
    "X_combined['Income_Per_Person'] = (X_combined['TotalIncome'] / X_combined['FamilySize']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff5211",
   "metadata": {},
   "source": [
    "### Convert continuous variables into categorical bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined[['TotalIncome']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning TotalIncome into categories\n",
    "\n",
    "# Define bins and labels for TotalIncome\n",
    "bins_income = [0, 3755.83, 7813.54, 11012.97, X_combined['TotalIncome'].max()]\n",
    "labels_income = ['Very Low', 'Low', 'Moderate', 'High']\n",
    "\n",
    "# Apply binning\n",
    "X_combined['TotalIncome_Bin'] = pd.cut(X_combined['TotalIncome'], bins=bins_income, labels=labels_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6331266",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined[['LoanAmount']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins and labels for LoanAmount\n",
    "bins_loan = [0, 40, 173, X_combined['LoanAmount'].max()]\n",
    "labels_loan = ['Very Small', 'Small', 'Large']\n",
    "\n",
    "# Apply binning\n",
    "X_combined['LoanAmount_Bin'] = pd.cut(X_combined['LoanAmount'], bins=bins_loan, labels=labels_loan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96df8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78355623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant columns from object to int\n",
    "\n",
    "X_combined['Married'] = X_combined['Married'].astype('int')\n",
    "X_combined['Gender'] = X_combined['Gender'].astype('int')\n",
    "X_combined['FamilySize'] = X_combined['FamilySize'].astype('int')\n",
    "X_combined['Education'] = X_combined['Education'].astype('int')\n",
    "X_combined['Self_Employed'] = X_combined['Self_Employed'].astype('int')\n",
    "X_combined['Credit_History'] = X_combined['Credit_History'].astype('int')\n",
    "X_combined['Dependents'] = X_combined['Dependents'].astype('int')\n",
    "X_combined['Property_Area'] = X_combined['Property_Area'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b068a569-ed55-4526-bc56-1f05424c8180",
   "metadata": {},
   "source": [
    "#### One hot encoding for selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc46244",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = pd.get_dummies(X_combined, columns=['TotalIncome_Bin', 'LoanAmount_Bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994cea3",
   "metadata": {},
   "source": [
    "### Drop columns that we don't need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc84ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined.drop(columns=['ID', 'Loan_ID'], inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbafa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined.drop(columns=['Married', 'Gender'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326148ce",
   "metadata": {},
   "source": [
    "### Split Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed78c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "X_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d76d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate combined data back into train and test sets\n",
    "X = X_combined[X_combined['source'] == 'train'].drop(columns=['source'])\n",
    "test_data_processed = X_combined[X_combined['source'] == 'test'].drop(columns=['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae89418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a08ad",
   "metadata": {},
   "source": [
    "### Oversampling\n",
    "Oversampling involves increasing the number of instances in the minority class to balance the dataset and improve the model's performance. It tries to solve the problem of imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86020fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "#Reduced model performance, so we ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a59584",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on the training data and transform both train and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "test_data_processed = scaler.transform(test_data_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d93fa3a",
   "metadata": {},
   "source": [
    "## 4. Modelling\n",
    "This section compares different classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f960bbb1",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e3e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "logistic_model = LogisticRegression(random_state=40, max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set and evaluate\n",
    "y_pred = logistic_model.predict(X_test_scaled)\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07413b8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "features = X_train.columns\n",
    "\n",
    "# Get coefficients\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': logistic_model.coef_[0]  # For binary classification\n",
    "})\n",
    "\n",
    "# Sort by absolute importance\n",
    "feature_importance_df['Absolute Importance'] = abs(feature_importance_df['Importance'])\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Absolute Importance', ascending=False)\n",
    "\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596bd2f6",
   "metadata": {},
   "source": [
    "#### 2. CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec2030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8006b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc801de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the CatBoost model\n",
    "catboost_model = CatBoostClassifier(iterations=100, random_seed=42, verbose=0)\n",
    "\n",
    "# Train the model\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and evaluate\n",
    "y_pred_catboost = catboost_model.predict(X_test)\n",
    "print(\"CatBoost Accuracy Score:\", accuracy_score(y_test, y_pred_catboost))\n",
    "\n",
    "# Classification report\n",
    "print(\"CatBoost Classification Report:\\n\", classification_report(y_test, y_pred_catboost))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"CatBoost Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_catboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0232e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "features = X_train.columns\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': catboost_model.get_feature_importance()\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86eabca",
   "metadata": {},
   "source": [
    "#### 3. AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AdaBoost model\n",
    "adaboost_model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and evaluate\n",
    "y_pred_adaboost = adaboost_model.predict(X_test)\n",
    "print(\"AdaBoost Accuracy Score:\", accuracy_score(y_test, y_pred_adaboost))\n",
    "\n",
    "# Classification report\n",
    "print(\"AdaBoost Classification Report:\\n\", classification_report(y_test, y_pred_adaboost))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"AdaBoost Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_adaboost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f541472a",
   "metadata": {},
   "source": [
    "#### 4. LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42282465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LightGBM model\n",
    "lightgb_model = LGBMClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lightgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set and evaluate\n",
    "y_pred = lightgb_model.predict(X_test_scaled)\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b042305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importances = lightgb_model.feature_importances_\n",
    "features = X_train.columns\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.barh(range(X_train.shape[1]), importances[indices], align=\"center\")  # Horizontal bar plot\n",
    "plt.yticks(range(X_train.shape[1]), [features[i] for i in indices])     # Labels on the y-axis\n",
    "plt.xlabel(\"Importance\")  # Label for the x-axis\n",
    "plt.ylabel(\"Features\")    # Label for the y-axis\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fbeb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': [features[i] for i in indices],\n",
    "    'Importance': importances[indices]\n",
    "})\n",
    "\n",
    "\n",
    "# Sort the DataFrame by importance for better visualization\n",
    "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Display sorted DataFrame\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecaef30-7807-4a9a-a7fd-b5eb07ee0469",
   "metadata": {},
   "source": [
    "#### 5. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f5dd4-360f-4383-b748-6abb416b33dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgboost_model = XGBClassifier(random_state=40)\n",
    "\n",
    "# Train the model\n",
    "xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and evaluate\n",
    "y_pred = xgboost_model.predict(X_test)\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ee539-a220-4d2d-8837-b21b7a542965",
   "metadata": {},
   "source": [
    "#### 6. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165684a9-477d-43dd-b555-b0ed6e08e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "randomforest_model = RandomForestClassifier(random_state=40)\n",
    "\n",
    "# Train the model\n",
    "randomforest_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and evaluate\n",
    "y_pred = randomforest_model.predict(X_test)\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be9c707-c981-4e9a-85de-5e55c9b6d631",
   "metadata": {},
   "source": [
    "#### 7.  Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655abd80-a26d-4f17-9e53-2b2058344d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the SVC model\n",
    "svc_model = SVC(random_state=40, max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "svc_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set and evaluate\n",
    "y_pred = svc_model.predict(X_test_scaled)\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8883a3",
   "metadata": {},
   "source": [
    "#### 8. Feed Forward Neural Networks (FNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c8e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.3),  # Dropout for regularization\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', # Metric to monitor\n",
    "                               patience=5,         # Number of epochs to wait for improvement\n",
    "                               restore_best_weights=True)  # Restore best model weights after training\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy',Precision(), Recall()])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32,\n",
    "                    validation_split=0.2, verbose=1,\n",
    "                     callbacks=[early_stopping])  # Apply early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa42e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print(f'Test Precision: {test_precision}')\n",
    "print(f'Test Recall: {test_recall}')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Display the Classification Report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Display the Confusion Matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb7efa",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning with RandomizedSearchCV\n",
    "This section demonstrates how to optimize model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the best performing model, CatBoost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Initialize the CatBoost model\n",
    "catboost_model = CatBoostClassifier(random_seed=42, verbose=0)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'depth': [2, 5, 8, 10],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2],\n",
    "    'iterations': [100, 300, 500, 1000, 1500],\n",
    "    'l2_leaf_reg': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'bagging_temperature': [0, 0.5, 1],\n",
    "    'border_count': [32, 64, 128]\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for hyperparameter tuning\n",
    "catboost_search = RandomizedSearchCV(\n",
    "    estimator=catboost_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    scoring='accuracy',  \n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "catboost_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = catboost_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad052d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CatBoost model with the best parameters\n",
    "optimized_catboost_model = CatBoostClassifier(**best_params, random_seed=42, verbose=0)\n",
    "optimized_catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_catboost = optimized_catboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Optimized CatBoost Accuracy Score:\", accuracy_score(y_test, y_pred_catboost))\n",
    "print(\"Optimized CatBoost Classification Report:\\n\", classification_report(y_test, y_pred_catboost))\n",
    "print(\"Optimized CatBoost Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_catboost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547cfa13",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CatBoost model\n",
    "catboost_model = CatBoostClassifier(iterations=100, random_seed=42, verbose=0)\n",
    "\n",
    "# Train the model on the entire dataset\n",
    "catboost_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c12b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the trained model\n",
    "predictions = catboost_model.predict(test_data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e05ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with required columns\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_data['ID'],\n",
    "    'Loan_Status': predictions  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef5ccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb409dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafd0ad-e3de-4373-92cf-f082233edd99",
   "metadata": {},
   "source": [
    "---\n",
    "_**Your Dataness**_,  \n",
    "**`Obinna Oliseneku`** (_**Hybraid**_)  \n",
    "**[LinkedIn](https://www.linkedin.com/in/obinnao/)** | **[GitHub](https://github.com/hybraid6)**  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
