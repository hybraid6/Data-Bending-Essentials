{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38a41c3",
   "metadata": {},
   "source": [
    "# Comprehensive Regression Workflow: Predicting Laptop Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765a89c1",
   "metadata": {},
   "source": [
    "## **1. Introduction**\n",
    "This notebook aims to build a **regression model to predict the price of laptops** (**Price_euros**) using their specifications, such as **RAM, CPU, Screen Size, and GPU**.  \n",
    "\n",
    "Regression is a supervised learning technique that estimates a **continuous target variable** (**price**) based on input features. By analyzing technical specifications, we aim to understand which factors influence laptop pricing the most.  \n",
    "\n",
    "This workflow will guide you through each step of the **laptop price prediction process**, including:\n",
    "\n",
    "- **Data Exploration**: Understanding the dataset structure and identifying key trends.  \n",
    "- **Preprocessing**: Handling missing values, encoding categorical variables, and scaling numerical features.  \n",
    "- **Feature Engineering**: Creating or modifying features to improve model performance.  \n",
    "- **Model Selection**: Comparing different regression models (Linear Regression, Ridge, Random Forest, etc.).  \n",
    "- **Hyperparameter Tuning**: Optimizing the best model for improved predictions.  \n",
    "- **Evaluation**: Assessing performance using metrics like **RMSE (Root Mean Squared Error)** and **R² (R-squared score)**.  \n",
    "\n",
    "By the end of this notebook, we will have a well-trained **regression model** capable of estimating laptop prices based on specifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69070579",
   "metadata": {},
   "source": [
    "## **2. Data Preprocessing**\n",
    "This section covers data cleaning, handling missing values, and preparing the dataset for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e2ca9",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee991da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8f0790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data manipulation and visualization\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import re  # Import the regular expressions module for pattern matching and text processing\n",
    "import matplotlib.pyplot as plt  # For plotting data\n",
    "import seaborn as sns  # For enhanced data visualizations\n",
    "\n",
    "# Import libraries for machine learning models and evaluation\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder  # For scaling numerical data and encoding categorical data\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet  # For linear Regression\n",
    "from sklearn.tree import DecisionTreeRegressor  # For Decision Tree Regression\n",
    "from sklearn.ensemble import RandomForestRegressor  # For Random Forest Regression\n",
    "from sklearn.svm import SVR  # For Support Vector Regression \n",
    "import xgboost as xgb # For XGBoost Regression\n",
    "from sklearn.model_selection import cross_validate  # To perform cross-validation\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer  # For model evaluation metrics\n",
    "from sklearn.model_selection import GridSearchCV   # For hyperparameter tuning\n",
    "\n",
    "\n",
    "# Set visual settings for plots (optional)\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = [10, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3cb8ea",
   "metadata": {},
   "source": [
    "#### Brief overview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf247c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"laptop_price.csv\", encoding=\"latin-1\")\n",
    "'''\n",
    "\"encoding='latin-1'\" is used to handle special characters in the dataset  \n",
    "This ensures that the dataset is read correctly, especially if it contains special characters like é, ñ, ü, etc.\n",
    "'''\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8d43c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display 10 random rows of the dataset\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0a10ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display a concise summary of the dataframe, including the number of non-null entries and the data type of each column\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413598ad",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- The dataset contains 1303 entries and 13 columns.\n",
    "- `laptop_ID` is an integer but is just an identifier (not useful for modeling).\n",
    "- Most columns are object (string) types, including `Ram`, `Memory`, and `Weight` which should be converted to numerical values.\n",
    "- `ScreenResolution`, `Cpu`, and `Gpu` may require feature extraction since they contain multiple pieces of information.\n",
    "- **No missing values** are detected, so we don't need to handle NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of numerical columns\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25fe986",
   "metadata": {},
   "source": [
    "#### Observations for numerical columns:\n",
    "- `laptop_ID` is an identifier and does not contribute to the regression model.\n",
    "- The average screen size (`Inches`) is ~15 inches, with a range from 10.1 to 18.4 inches.\n",
    "- `Price_euros` varies significantly, with a minimum price of €174 and a maximum of €6099.\n",
    "- The median price (\\~€977) is lower than the mean (~€1123), indicating a possible right-skewed distribution. We may need to transform later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba0944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summary statistics of categorical columns\n",
    "data.describe(include = 'object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d8337c",
   "metadata": {},
   "source": [
    "#### Observations for categorical columns:\n",
    "- There are 19 unique laptop brands (`Company`), with Dell being the most common.\n",
    "- The Product column contains 618 unique values out of 1303 rows, meaning it's highly granular and behaves almost like an identifier rather than a meaningful feature; so we'll drop it.\n",
    "- `TypeName` shows that most laptops are Notebooks (727 out of 1303).\n",
    "- `ScreenResolution` has 40 unique values, suggesting feature extraction might be needed.\n",
    "- `Cpu`, `Gpu`, and `Memory` have high cardinality (many unique values), requiring encoding or feature engineering.\n",
    "- `OpSys` is dominated by `Windows 10` (1072 occurrences), so we might consider grouping less common OS types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f39527",
   "metadata": {},
   "source": [
    "#### Drop product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dfd90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop(\"Product\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd206a9f",
   "metadata": {},
   "source": [
    "#### Convert data types of `Ram` and `Weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5266a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"Ram\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the \"GB\" suffix from the 'Ram' column and convert it to an integer\n",
    "df[\"Ram\"] = df[\"Ram\"].str.replace(\"GB\", \"\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c25c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Weight\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the \"kg\" suffix from the 'Weight' column and convert it to a float\n",
    "df[\"Weight\"] = df[\"Weight\"].str[:-2].astype(float)\n",
    "# \".str[:-2]\" removes the last two characters (\"kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f8b85",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "Feature Engineering is a technique by which we create new features that could potentially aid in predicting our target variable, which in this case, is laptop price. In this notebook, we will create additional features based on our **Domain Knowledge** of the laptop features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20a9e88",
   "metadata": {},
   "source": [
    "#### Extract screen resolution width & height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe969e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for inconsistent formats in ScreenResolution\n",
    "df[\"ScreenResolution\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ada8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pattern (\\d+)x(\\d+) captures two groups of digits separated by 'x', e.g., \"1920x1080\"\n",
    "df[[\"ScreenWidth\", \"ScreenHeight\"]] = df[\"ScreenResolution\"].str.extract(r\"(\\d+)x(\\d+)\").astype(int)\n",
    "\n",
    "# Drop the original 'ScreenResolution' column since its information is now split into two separate columns\n",
    "df.drop(columns=[\"ScreenResolution\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243acba",
   "metadata": {},
   "source": [
    "#### Extract brand and frequency from `Cpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bf79a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"Cpu\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8248e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the brand of the CPU (first word in the 'Cpu' column)\n",
    "# Example: \"Intel Core i5 7200U 2.5GHz\" → \"Intel\"\n",
    "df[\"CPU Brand\"] = df.Cpu.str.split(\" \").apply(lambda x: x[0])\n",
    "\n",
    "\n",
    "# Extract the CPU frequency (last element in the 'Cpu' column)\n",
    "# Example: \"Intel Core i5 7200U 2.5GHz\" → \"2.5GHz\"\n",
    "df[\"CPU Frequency\"] = df.Cpu.str.split(\" \").apply(lambda x: x[-1])\n",
    "\n",
    "# Remove 'GHz' and convert CPU Frequency to a numeric format\n",
    "df[\"CPU Frequency\"] = df[\"CPU Frequency\"].str[:-3].astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Cpu\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e009b",
   "metadata": {},
   "source": [
    "#### Extract memory amount and type from `Memory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0276f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Memory\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e89af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert memory size to MB\n",
    "def convert_memory_to_MB(memory_str):\n",
    "    \"\"\"\n",
    "    Converts memory sizes (GB, TB) into MB.\n",
    "    Handles cases where storage types are included (e.g., '128GB SSD').\n",
    "    Handles multiple storage types correctly (e.g., '256GB SSD + 1TB HDD').\n",
    "    \"\"\"\n",
    "    total_memory = 0  # Initialize total storage size\n",
    "    \n",
    "    # Split in case there are multiple storage types\n",
    "    for mem in memory_str.split(\"+\"):\n",
    "        mem = mem.strip()  # Remove unnecessary spaces\n",
    "        \n",
    "        # Extract numeric value using regex\n",
    "        match = re.findall(r\"(\\d+\\.?\\d*)\", mem)  # Finds numbers (including decimals)\n",
    "        if match:\n",
    "            size = float(match[0])  # Convert extracted number to float\n",
    "            \n",
    "            # Convert to MB based on unit (GB or TB) 1GB = 1000MB; 1TB = 1,000,000MB\n",
    "            if \"GB\" in mem:\n",
    "                total_memory += size * 1000\n",
    "            elif \"TB\" in mem:\n",
    "                total_memory += size * 1000000\n",
    "    \n",
    "    return total_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9161f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract storage type (SSD, HDD, etc.)\n",
    "def extract_memory_type(memory_str):\n",
    "    \"\"\"\n",
    "    Extracts the storage type (SSD, HDD, Hybrid, Flash Storage).\n",
    "    If multiple storage types exist, it returns all types found.\n",
    "    \"\"\"\n",
    "    types = []\n",
    "    \n",
    "    for mem in memory_str.split(\"+\"):\n",
    "        mem = mem.strip()\n",
    "        if \"SSD\" in mem:\n",
    "            types.append(\"SSD\")\n",
    "        elif \"HDD\" in mem:\n",
    "            types.append(\"HDD\")\n",
    "        elif \"Hybrid\" in mem:\n",
    "            types.append(\"Hybrid\")\n",
    "        elif \"Flash Storage\" in mem:\n",
    "            types.append(\"Flash Storage\")\n",
    "    \n",
    "    return \" + \".join(types)  # Combine types if multiple exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d994bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply functions to transform memory data\n",
    "df[\"Total Memory (MB)\"] = df[\"Memory\"].apply(convert_memory_to_MB)\n",
    "df[\"Memory Type\"] = df[\"Memory\"].apply(extract_memory_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc609ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Memory Type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f13ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original 'Memory' column\n",
    "df.drop(columns=[\"Memory\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e2e31",
   "metadata": {},
   "source": [
    "#### Extract the brand name from the GPU column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293c885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Gpu\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the brand name by splitting the string and taking the first word\n",
    "df[\"GPU Brand\"] = df.Gpu.str.split(\" \").apply(lambda x: x[0])\n",
    "\n",
    "# Drop the original 'Gpu' column\n",
    "df = df.drop(\"Gpu\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e94d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the processed DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27422fb",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "This section includes visualizations and insights to understand the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380a334",
   "metadata": {},
   "source": [
    "### Univariate Analysis\n",
    "Univariate analysis involves analyzing individual features one at a time. This helps to understand the distribution, central tendency, and variability of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fcfee8",
   "metadata": {},
   "source": [
    "#### Visualizing Price Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b05812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size to 8x5 inches for better visibility\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Create a histogram to visualize the distribution of laptop prices\n",
    "# 'bins=30' ensures the data is divided into 30 intervals\n",
    "# 'kde=True' adds a Kernel Density Estimate (KDE) line to show the smooth probability distribution\n",
    "sns.histplot(df[\"Price_euros\"], bins=30, kde=True, color=\"teal\")\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title(\"Distribution of Laptop Prices\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eee20f",
   "metadata": {},
   "source": [
    "Since the price distribution is right-skewed with very few items above 4000, we'll normalize later using  Log Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3168837",
   "metadata": {},
   "source": [
    "#### Distribution of other numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84829f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = df.select_dtypes(include='number').columns\n",
    "len(numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf8ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1d635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(0, len(numerical_features)):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.boxplot(x = df[numerical_features[i]], palette = 'viridis')\n",
    "    plt.title(numerical_features[i], fontsize = 15)\n",
    "    plt.xlabel(' ')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff4d73f",
   "metadata": {},
   "source": [
    "#### Distribution of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(include='object').columns\n",
    "len(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6630f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(0, len(categorical_features)):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.countplot(x = df[categorical_features[i]], palette = 'viridis')\n",
    "    plt.title(categorical_features[i], fontsize = 10)\n",
    "    plt.xlabel(' ')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cd4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the value counts of each categorical feature\n",
    "for i in range(0, len(categorical_features)):\n",
    "    # Print the value counts for each categorical feature\n",
    "    print(f\"Value counts for {categorical_features[i]}:\")\n",
    "    print(df[categorical_features[i]].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2689749b",
   "metadata": {},
   "source": [
    "- We will drop `CPU Brand` and `OpSys` as they predominantly contain one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749c8d2",
   "metadata": {},
   "source": [
    "### Bi-Variate Analysis\n",
    "Bi-variate analysis looks at 2 different features to identify any possible relationship or distinctive patterns between the 2 features. We are going to compare all the features with the target variable `Price_euros`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc42261",
   "metadata": {},
   "source": [
    "#### Feature Correlation - for numerical columns\n",
    "\n",
    "- One of the commonly used techniques for Bi-variate analysis between numerical values is the  **Correlation Matrix**. Correlation matrix is an effective tool to uncover linear relationship (Correlation) between any 2 continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbeaa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation matrix of numerical features\n",
    "plt.figure(figsize=(24, 10))\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e32238",
   "metadata": {},
   "source": [
    "- We can see the the correlations with the Target variable on the 5th row\n",
    "- ScreenWidth and ScreenHeight have a very high correlation (0.99) which might negatively affect the performance of models that are sensitive to multicollinearity. So we'll go with just one of them ( `ScreenWidth`)\n",
    "- We may also want to drop `Inches` and of course `laptop_ID` as they have very weak correlation with the price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc575a4",
   "metadata": {},
   "source": [
    "#### Categorical Columns vs Laptop prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab592ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot: SalePrice distribution across different categories\n",
    "plt.figure(figsize=(10, 15))  # Reduce figure size for better visibility\n",
    "for i in range(0, len(categorical_features)):\n",
    "    plt.subplot(3, 2, i+1)  # Adjust grid to 2x3 (or whatever fits best)\n",
    "    sns.boxplot(x=categorical_features[i], y='Price_euros', data=df, palette='viridis')\n",
    "    plt.title(f'Laptop Price vs. {categorical_features[i]}', fontsize=15)\n",
    "    plt.xlabel(categorical_features[i], fontsize=12)\n",
    "    plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "    plt.ylabel('Laptop Price', fontsize=12)  # Add y-axis label for clarity\n",
    "\n",
    "# Apply tight_layout after all subplots are created\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60845cc",
   "metadata": {},
   "source": [
    "Here we can check whether the distribution of price between different categories are distinct enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99ea9d",
   "metadata": {},
   "source": [
    "## 5. Modelling\n",
    "Here, we split our data, scale and compare different regression models. We will also try out hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705698d",
   "metadata": {},
   "source": [
    "#### Encode categorical variables\n",
    "Because machine learning only learns from data that is numerical in nature, we will convert the categorical columns into numerical columns (*one-hot features*) using the `get_dummies()` method that are suitable for feeding into our machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa99b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns to be encoded\n",
    "categorical_cols = [\"Company\", \"TypeName\", \"GPU Brand\", \"Memory Type\"]\n",
    "\n",
    "# Apply one-hot encoding to the categorical columns\n",
    "# - pd.get_dummies creates binary columns for each category in the categorical columns\n",
    "df = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# Set option to display all the columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame to check the encoding\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772387d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b03a1b87",
   "metadata": {},
   "source": [
    "#### Laptop Price Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43288f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram to visualize the distribution of laptop prices before transformation\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Before transformation of Laptop Price\")\n",
    "sns.histplot(df[\"Price_euros\"], bins=30, kde=True, color=\"teal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6e9a7",
   "metadata": {},
   "source": [
    "The distribution is skewed to the right, where the tail on the curve’s right-hand side is longer than the tail on the left-hand side, and the mean is greater than the mode. This situation is also called positive skewness.  \n",
    "Having a skewed target will affect the overall performance of our machine learning model, thus, one way to alleviate will be to use **log transformation** on the skewed target, in our case, the *Price_euros* to reduce the skewness of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram to visualize the distribution of log-transformed laptop prices\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"After transformation of Laptop Price\")\n",
    "sns.histplot(np.log(df[\"Price_euros\"]), bins=30, kde=True, color=\"teal\") # Apply the natural logarithm to the prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2102e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the log transformation\n",
    "df[\"Price_euros\"] = np.log(df[\"Price_euros\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc5bc6",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad11b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns and separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"Price_euros\", \"laptop_ID\", \"OpSys\", \"CPU Brand\",\"Inches\",\"ScreenHeight\"])  # Features\n",
    "y = df[\"Price_euros\"]  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf7fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d39c0b",
   "metadata": {},
   "source": [
    "#### Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04748f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5e267",
   "metadata": {},
   "source": [
    "#### Scaling the data\n",
    "- Scaling ensures that each feature contributes equally to the distance calculations or the optimization process. \n",
    "- We'll use **Standardization** here.  Standardization transforms the features to have a mean of 0 and a standard deviation of 1. This is useful when the features have different units or scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a77d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757204a1",
   "metadata": {},
   "source": [
    "#### 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6969bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize the model\n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "linear_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_linear = linear_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_test, y_pred_linear))\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "\n",
    "print(f\"Linear Regression:\\n RMSE: {rmse_linear:.4f}\\n R²: {r2_linear:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac865e6b",
   "metadata": {},
   "source": [
    "#### 2. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66156b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Initialize the model\n",
    "ridge_reg = Ridge(alpha=1.0)\n",
    "\n",
    "# Fit the model to the training data\n",
    "ridge_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_ridge = ridge_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(f\"Ridge Regression:\\n RMSE: {rmse_ridge:.4f}\\n R²: {r2_ridge:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41116c73",
   "metadata": {},
   "source": [
    "#### 3. Elastic Net regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Initialize the Elastic Net model\n",
    "elastic_net = ElasticNet(alpha=0.1)\n",
    "\n",
    "# Fit the model\n",
    "elastic_net.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicting on test data\n",
    "y_pred_en = elastic_net.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_en = np.sqrt(mean_squared_error(y_test, y_pred_en))\n",
    "r2_en = r2_score(y_test, y_pred_en)\n",
    "\n",
    "print(f\"Elastic Net Regression:\\n RMSE: {rmse_en:.4f}\\n R²: {r2_en:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631351a",
   "metadata": {},
   "source": [
    "#### 4. Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cac9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Initialize the model\n",
    "svr_reg = SVR(kernel='linear')\n",
    "\n",
    "# Fit the model to the training data\n",
    "svr_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_svr = svr_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_svr = np.sqrt(mean_squared_error(y_test, y_pred_svr))\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "print(f\"Support Vector Regression:\\n RMSE: {rmse_svr:.4f}\\n R²: {r2_svr:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e2ccc3",
   "metadata": {},
   "source": [
    "#### 5. K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Initialize the model\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "knn_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_knn = knn_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_knn = np.sqrt(mean_squared_error(y_test, y_pred_knn))\n",
    "r2_knn = r2_score(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"K-Nearest Neighbors:\\n RMSE: {rmse_knn:.4f}\\n R²: {r2_knn:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b3823",
   "metadata": {},
   "source": [
    "#### 6. Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195aaa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize the model\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "\n",
    "# Fit the model to the training data\n",
    "tree_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_tree = tree_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_tree = np.sqrt(mean_squared_error(y_test, y_pred_tree))\n",
    "r2_tree = r2_score(y_test, y_pred_tree)\n",
    "\n",
    "print(f\"Decision Tree Regressor:\\n RMSE: {rmse_tree:.4f}\\n R²: {r2_tree:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd6510",
   "metadata": {},
   "source": [
    "#### 7. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize the model\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "# Fit the model to the training data\n",
    "forest_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_forest = forest_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_forest = np.sqrt(mean_squared_error(y_test, y_pred_forest))\n",
    "r2_forest = r2_score(y_test, y_pred_forest)\n",
    "\n",
    "print(f\"Random Forest Regressor:\\n RMSE: {rmse_forest:.4f}\\n R²: {r2_forest:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af19f75",
   "metadata": {},
   "source": [
    "#### 8. Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ec0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Initialize the model\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "\n",
    "# Fit the model to the training data\n",
    "gb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_gb = gb_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "print(f\"Gradient Boosting Regressor:\\n RMSE: {rmse_gb:.4f}\\n R²: {r2_gb:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029bfd0",
   "metadata": {},
   "source": [
    "#### 9. XGBoost (eXtreme Gradient Boosting) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba2ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install lightgbm\n",
    "!pip install xgboost\n",
    "\n",
    "# Importing LightGBM\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with some parameters\n",
    "xgb_reg = xgb.XGBRegressor()\n",
    "\n",
    "# Fit the model\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on test data\n",
    "y_pred_xgb = xgb_reg.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost Regressor:\\n RMSE: {rmse_xgb:.4f}\\n R²: {r2_xgb:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d2cbd3",
   "metadata": {},
   "source": [
    "#### 10. LightGBM (Light Gradient Boosting Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install lightgbm\n",
    "!pip install lightgbm\n",
    "\n",
    "# Importing LightGBM\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ae582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba68d41",
   "metadata": {},
   "source": [
    "#### Select the best performing model with a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e972f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize regression models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "    \"ElasticNet Regression\": ElasticNet(alpha=0.1),\n",
    "    \"Support Vector Regression\": SVR(kernel='linear'),\n",
    "    \"Gradient Boosting Regressor\": GradientBoostingRegressor(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsRegressor(n_neighbors=5),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(),\n",
    "    \"XGBoost Regressor\": xgb.XGBRegressor()\n",
    "}\n",
    "\n",
    "# Initializing Best Model Trackers\n",
    "best_model = None\n",
    "best_rmse = float(\"inf\") # Set RMSE to a very high value initially (infinity)\n",
    "best_r2 = float(\"-inf\")  # Set R² to a very low value initially (-infinity)\n",
    "\n",
    "\n",
    "# Train each model and evaluate its performance\n",
    "for name, model in models.items():\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate the model using RMSE and R²\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{name}:\\n RMSE: {rmse:.4f}\\n R²: {r2:.4f}\\n\")\n",
    "    \n",
    "    # Select the best model based on RMSE\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_r2 = r2\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\\n Best RMSE: {best_rmse:.4f}\\n Best R²: {best_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2676f604",
   "metadata": {},
   "source": [
    "#### Evaluating With Cross Validation\n",
    "- Cross-validation is a method to evaluate a model by splitting the data into multiple parts, training and testing the model on different subsets in each round, and then averaging the results. This helps ensure the model's performance is reliable and not just specific to one split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f322f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost model\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Create custom scorers for RMSE and R²\n",
    "# `make_scorer` allows using custom metrics in cross-validation\n",
    "rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "# Dictionary of scoring metrics\n",
    "scoring = {'RMSE': rmse_scorer, 'R2': r2_scorer}\n",
    "\n",
    "# Perform cross-validation\n",
    "# `cross_validate` splits the data into multiple folds, trains and tests the model, and calculates the scores.\n",
    "# Since cross-validation already includes multiple train-test splits, we use the full dataset (X, y).\n",
    "cv_results = cross_validate(model, X, y, scoring=scoring, cv=5, return_train_score=True)\n",
    "\n",
    "# Output the results\n",
    "# `cv_results` contains the scores for each fold\n",
    "print(\"RMSE scores:\", cv_results['test_RMSE'])  # RMSE scores for each fold\n",
    "print(\"R² scores:\", cv_results['test_R2'])  # R² scores for each fold\n",
    "print(\"Average RMSE:\", cv_results['test_RMSE'].mean())  # Average RMSE across all folds\n",
    "print(\"Average R²:\", cv_results['test_R2'].mean())  # Average R² score across all folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c366223",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning\n",
    "- Hyperparameter tuning is the process of finding the best *settings* for a machine learning model to improve its performance.\n",
    "- Think of it like adjusting the knobs on a machine to make it work better. \n",
    "- You test different combinations of settings (hyperparameters) and evaluate how well the model performs with each set. \n",
    "- The goal is to find the combination that leads to the best results, such as higher accuracy or lower error rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cdbdd1",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning with Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab647080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use XGBoost and tune the n_estimators, learning_rate, and max_depth.\n",
    "\n",
    "# Initialize the best RMSE and best R² to extreme values to ensure any calculated values will be better\n",
    "best_rmse = float('inf')\n",
    "best_r2 = -float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Iterate over different values for n_estimators, learning_rate, and max_depth\n",
    "for n_estimators in [50, 100, 200]:  # Number of boosting rounds\n",
    "    for learning_rate in [0.01, 0.1, 0.2]:  # Step size at each iteration\n",
    "        for max_depth in range(3, 10, 2):  # Maximum depth of each tree\n",
    "\n",
    "            # Initialize the XGBoost model with the current set of hyperparameters\n",
    "            xgb_reg = xgb.XGBRegressor(\n",
    "                n_estimators=n_estimators,       # Number of boosting rounds\n",
    "                learning_rate=learning_rate,     # Step size at each iteration\n",
    "                max_depth=max_depth,             # Maximum depth of each tree\n",
    "                random_state=42                  # Ensures reproducibility\n",
    "            )\n",
    "            \n",
    "            # Train the model using the training data\n",
    "            xgb_reg.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict the target values for the test data\n",
    "            y_pred_xgb = xgb_reg.predict(X_test)\n",
    "            \n",
    "            # Calculate Root Mean Squared Error (RMSE) and R² score for the current model\n",
    "            rmse = mean_squared_error(y_test, y_pred_xgb, squared=False)\n",
    "            r2 = r2_score(y_test, y_pred_xgb)\n",
    "            \n",
    "            # Check if the current RMSE is better (lower) than the best RMSE so far\n",
    "            if rmse < best_rmse:\n",
    "                # Update the best RMSE, R² score, and the best parameters\n",
    "                best_rmse = rmse\n",
    "                best_r2 = r2\n",
    "                best_params = {\n",
    "                    'n_estimators': n_estimators, \n",
    "                    'learning_rate': learning_rate, \n",
    "                    'max_depth': max_depth\n",
    "                }\n",
    "\n",
    "# Print the best hyperparameters and corresponding RMSE and R² score\n",
    "print(\"Best Parameters for XGBoost:\", best_params)\n",
    "print(\"Best RMSE for XGBoost:\", best_rmse)\n",
    "print(\"Best R² Score for XGBoost:\", best_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba924f",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning with `GridSearchCV` (GridSearch Cross Validation)\n",
    "- `GridSearchCV` is a technique used to optimize machine learning models by systematically evaluating all possible combinations of specified hyperparameters.\n",
    "- `GridSearchCV` combines grid search and cross-validation into a single tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa740ea",
   "metadata": {},
   "source": [
    "#####   This will take some time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d73cc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the XGBoost model with default settings\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],       # Number of boosting rounds\n",
    "    'learning_rate': [0.01, 0.1, 0.2],    # Step size at each iteration\n",
    "    'max_depth': [3, 5, 7, 9]             # Maximum depth of each tree\n",
    "}\n",
    "\n",
    "# Define custom scorers for RMSE and R²\n",
    "rmse_scorer = make_scorer(mean_squared_error,  greater_is_better=False, squared=False)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "# Initialize GridSearchCV with multiple scoring metrics\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,                 # Model to use\n",
    "    param_grid=param_grid,               # Parameter grid to search\n",
    "    scoring={'RMSE': rmse_scorer, 'R2': r2_scorer},  # Metrics to evaluate\n",
    "    cv=5,                                # Number of cross-validation folds\n",
    "    refit = 'RMSE',                       # Metric to optimize\n",
    "    verbose=1,                           # Level of verbosity for output\n",
    "    n_jobs= -1                            # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_rmse = -grid_search.best_score_  # GridSearchCV minimizes the score\n",
    "best_r2 = grid_search.cv_results_['mean_test_R2'][grid_search.best_index_]\n",
    "\n",
    "# Print the best hyperparameters and corresponding RMSE and R² score\n",
    "print(\"Best Parameters for XGBoost:\", best_params)\n",
    "print(\"Best RMSE for XGBoost:\", best_rmse)\n",
    "print(\"Best R² Score for XGBoost:\", best_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b44b6",
   "metadata": {},
   "source": [
    "#### Predict with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599579d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost model with best parameters\n",
    "best_xgb_model = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "# Train the model on the full training dataset\n",
    "best_xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914234fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE and R² on test data\n",
    "test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"Test R² Score: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d44dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "xgb.plot_importance(best_xgb_model, max_num_features=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe6c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model for Future Use\n",
    "import joblib\n",
    "\n",
    "joblib.dump(best_xgb_model, \"best_xgboost_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a8b3b",
   "metadata": {},
   "source": [
    "---\n",
    "***Your Dataness***,  \n",
    "Obinna Oliseneku (_**Hybraid**_)  \n",
    "**[LinkedIn](https://www.linkedin.com/in/obinnao/)** | **[GitHub](https://github.com/hybraid6)**  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
